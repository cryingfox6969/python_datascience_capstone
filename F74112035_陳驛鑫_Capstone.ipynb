{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone_F74112035\n",
    "\n",
    "This notebook will be your project submission. All tasks will be listed in the order of the Courses that they appear in. The tasks will be the same as in the Capstone Example Notebook, but in this submission you ___MUST___ use another dataset. Failure to do so will result in a large penalty to your grade in this course.\n",
    "\n",
    "## Finding your dataset\n",
    "\n",
    "Take some time to find an interesting dataset! There is a reading discussing various places where datasets can be found, but if you are able to process it, go ahead and use it! Do note, for some tasks in this project, each entry will need 3+ attributes, so keep that in mind when finding datasets. After you have found your dataset, the tasks will continue as in the Example Notebook. You will be graded based on the tasks and your results. Best of luck!\n",
    "\n",
    "### As Reviewer: \n",
    "Your job will be to verify the calculations made at each \"TODO\" labeled throughout the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Step: Imports\n",
    "\n",
    "In the next cell we will give you all of the imports you should need to do your project. Feel free to add more if you would like, but these should be sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import numpy\n",
    "import scipy.optimize\n",
    "import string\n",
    "from sklearn import linear_model\n",
    "from nltk.stem.porter import PorterStemmer # Stemming\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Data Processing\n",
    "\n",
    "### TODO 1: Read the data and Fill your dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由於課程附件內容有包括Submission跟example內容，所以在後面Task中會盡量兩個檔案的內容都包括到\n",
    "example file expression:\n",
    "Take care of int casting the votes and rating. Also __add this bit of code__ to your for loop, taking off the outer \" \":\n",
    "\n",
    "\" d['verified_purchase'] = d['verified_purchase'] == 'Y' \"\n",
    "\n",
    "This simple makes the verified purchase column be strictly true/false values rather than Y/N strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  marketplace  customer_id       review_id  product_id  product_parent  \\\n",
      "0          US     45610553   RMDCHWD0Y5OZ9  B00HH62VB6       618218723   \n",
      "1          US     14640079   RZSL0BALIYUNU  B003LRN53I       986692292   \n",
      "2          US      6111003   RIZR67JKUDBI0  B0006VMBHI       603261968   \n",
      "3          US      1546619  R27HL570VNL85F  B002B55TRG       575084461   \n",
      "4          US     12222213  R34EBU9QDWJ1GD  B00N1YPXW2       165236328   \n",
      "\n",
      "                                       product_title     product_category  \\\n",
      "0  AGPtek® 10 Isolated Output 9V 12V 18V Guitar P...  Musical Instruments   \n",
      "1         Sennheiser HD203 Closed-Back DJ Headphones  Musical Instruments   \n",
      "2                   AudioQuest LP record clean brush  Musical Instruments   \n",
      "3      Hohner Inc. 560BX-BF Special Twenty Harmonica  Musical Instruments   \n",
      "4        Blue Yeti USB Microphone - Blackout Edition  Musical Instruments   \n",
      "\n",
      "   star_rating  helpful_votes  total_votes vine verified_purchase  \\\n",
      "0            3              0            1    N                 N   \n",
      "1            5              0            0    N                 Y   \n",
      "2            3              0            1    N                 Y   \n",
      "3            5              0            0    N                 Y   \n",
      "4            5              0            0    N                 Y   \n",
      "\n",
      "                                     review_headline  \\\n",
      "0                                        Three Stars   \n",
      "1                                         Five Stars   \n",
      "2                                        Three Stars   \n",
      "3  I purchase these for a friend in return for pl...   \n",
      "4                                         Five Stars   \n",
      "\n",
      "                                         review_body review_date  \n",
      "0        Works very good, but induces ALOT of noise.  2015-08-31  \n",
      "1             Nice headphones at a reasonable price.  2015-08-31  \n",
      "2                       removes dust. does not clean  2015-08-31  \n",
      "3  I purchase these for a friend in return for pl...  2015-08-31  \n",
      "4                            This is an awesome mic!  2015-08-31  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 904004 entries, 0 to 904003\n",
      "Data columns (total 15 columns):\n",
      " #   Column             Non-Null Count   Dtype \n",
      "---  ------             --------------   ----- \n",
      " 0   marketplace        904004 non-null  object\n",
      " 1   customer_id        904004 non-null  int64 \n",
      " 2   review_id          904004 non-null  object\n",
      " 3   product_id         904004 non-null  object\n",
      " 4   product_parent     904004 non-null  int64 \n",
      " 5   product_title      904003 non-null  object\n",
      " 6   product_category   904004 non-null  object\n",
      " 7   star_rating        904004 non-null  int64 \n",
      " 8   helpful_votes      904004 non-null  int64 \n",
      " 9   total_votes        904004 non-null  int64 \n",
      " 10  vine               904004 non-null  object\n",
      " 11  verified_purchase  904004 non-null  object\n",
      " 12  review_headline    903998 non-null  object\n",
      " 13  review_body        903941 non-null  object\n",
      " 14  review_date        903996 non-null  object\n",
      "dtypes: int64(5), object(10)\n",
      "memory usage: 103.5+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 匯入資料集\n",
    "file_path = 'amazon_reviews_us_Musical_Instruments_v1_00.tsv'\n",
    "\n",
    "# 使用 sep='\\t' 明確指定分隔符\n",
    "data = pd.read_csv(\n",
    "    file_path,\n",
    "    sep='\\t',  # 指定制表符為分隔符\n",
    "    on_bad_lines='skip',  # 跳過解析錯誤的部分\n",
    "    low_memory=False  # 增加內存使用來處理大文件(not nessary)\n",
    ")\n",
    "\n",
    "print(data.head())\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 2: Split the data into a Training and Testing set\n",
    "\n",
    "First shuffle your data, then split your data. Have Training be the first 80%, and testing be the remaining 20%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "723203 180801\n"
     ]
    }
   ],
   "source": [
    "#YOUR CODE HERE\n",
    "# Shuffle the data\n",
    "shuffled_data = data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Split the data (80% training, 20% testing)\n",
    "split_index = int(len(shuffled_data) * 0.8)\n",
    "train_data = shuffled_data[:split_index]\n",
    "test_data = shuffled_data[split_index:]\n",
    "\n",
    "print(len(train_data), len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now delete your dataset\n",
    "You don't want any of your answers to come from your original dataset any longer, but rather your Training Set, this will help you to not make any mistakes later on, especialy when referencing the checkpoint solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE\n",
    "# Delete the original dataset to avoid referencing it\n",
    "del data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 3: Extracting Basic Statistics\n",
    "\n",
    "Next you need to answer some questions through any means (i.e. write a function or just find the answer) all based on the __Training Set:__\n",
    "1. How many entries are in your dataset?\n",
    "2. Pick a non-trivial attribute (i.e. verified purchases in example), what percentage of your data has this atttribute?\n",
    "3. Pick another different non-trivial attribute, what percentage of your data share both attributes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "question from example file:\n",
    "1. What is the __average rating__?\n",
    "2. What fraction of reviews are from __verified purchases__?\n",
    "3. How many __total users__ are there?\n",
    "4. How many __total items__ are there?\n",
    "5. What fraction of reviews have __5-star ratings__?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total entries in the training set: 723203\n",
      "Percentage of verified purchases: 86.37%\n",
      "Percentage of high ratings (>=4) and verified purchases: 70.53%\n",
      "Average rating: 4.25\n",
      "Fraction of reviews from verified purchases: 86.37%\n",
      "Total users: 481940\n",
      "Total items: 111046\n",
      "Fraction of reviews with 5-star ratings: 63.34%\n"
     ]
    }
   ],
   "source": [
    "# 確認訓練集大小\n",
    "total_entries = len(train_data)\n",
    "print(\"Total entries in the training set:\", total_entries)\n",
    "\n",
    "# 計算已驗證購買的比例\n",
    "verified_percentage = (train_data['verified_purchase'] == 'Y').mean() * 100\n",
    "print(f\"Percentage of verified purchases: {verified_percentage:.2f}%\")\n",
    "\n",
    "# 計算高評分且已驗證購買的比例\n",
    "high_rating_verified = ((train_data['verified_purchase'] == 'Y') & (train_data['star_rating'] >= 4)).mean() * 100\n",
    "print(f\"Percentage of high ratings (>=4) and verified purchases: {high_rating_verified:.2f}%\")\n",
    "\n",
    "# 計算平均評分\n",
    "average_rating = train_data['star_rating'].mean()\n",
    "print(f\"Average rating: {average_rating:.2f}\")\n",
    "\n",
    "# 計算已驗證購買評論的比例\n",
    "verified_fraction = (train_data['verified_purchase'] == 'Y').mean()\n",
    "print(f\"Fraction of reviews from verified purchases: {verified_fraction:.2%}\")\n",
    "\n",
    "# 計算唯一用戶數\n",
    "total_users = train_data['customer_id'].nunique()\n",
    "print(f\"Total users: {total_users}\")\n",
    "\n",
    "# 計算唯一商品數\n",
    "total_items = train_data['product_id'].nunique()\n",
    "print(f\"Total items: {total_items}\")\n",
    "\n",
    "# 計算 5 星級評分評論的比例\n",
    "five_star_fraction = (train_data['star_rating'] == 5).mean()\n",
    "print(f\"Fraction of reviews with 5-star ratings: {five_star_fraction:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Classification\n",
    "\n",
    "Next you will use our knowledge of classification to extract features and make predictions based on them. Here you will be using a Logistic Regression Model, keep this in mind so you know where to get help from.\n",
    "\n",
    "### TODO 1: Define the feature function\n",
    "\n",
    "This implementation will be based on ___any two___ attributes from your dataset. You will be using these two attributes to predict a third. Hint: Remember the offset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the feature function\n",
    "def feature(d):\n",
    "    \"\"\"\n",
    "    Extract features from a single row of data.\n",
    "    Attributes used:\n",
    "    - star_rating (numerical)\n",
    "    - review_body (length of the text)\n",
    "    \"\"\"\n",
    "    feat = [1, d['star_rating'], len(d['review_body'])]\n",
    "    return feat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 2: Fit your model\n",
    "\n",
    "1. Create your __Feature Vector__ based on your feature function defined above. \n",
    "2. Create your __Label Vector__ based on the \"verified purchase\" column of your training set.\n",
    "3. Define your model as a __Logistic Regression__ model.\n",
    "4. Fit your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained successfully.\n"
     ]
    }
   ],
   "source": [
    "#YOUR CODE HERE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# 定義特徵提取函數\n",
    "def feature(d):\n",
    "    \"\"\"\n",
    "    Extract features from a single row of data.\n",
    "    Attributes used:\n",
    "    - star_rating (numerical)\n",
    "    - review_body (length of the text)\n",
    "    \"\"\"\n",
    "    review_body_length = len(d['review_body']) if pd.notna(d['review_body']) else 0\n",
    "    feat = [1, d['star_rating'], review_body_length]\n",
    "    return feat\n",
    "\n",
    "# 提取特徵和標籤向量 (訓練集)\n",
    "X_train = train_data.apply(lambda d: feature(d), axis=1).tolist()\n",
    "y_train = (train_data['verified_purchase'] == 'Y').astype(int).tolist()\n",
    "\n",
    "# 訓練邏輯回歸模型\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Model trained successfully.\")\n",
    "#print(model.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 3: Compute Accuracy of Your Model\n",
    "\n",
    "1. Make __Predictions__ based on your model.\n",
    "2. Compute the __Accuracy__ of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the Logistic Regression Model: 0.864840349334351\n"
     ]
    }
   ],
   "source": [
    "#YOUR CODE HERE\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 提取特徵和標籤向量 (測試集)\n",
    "X_test = test_data.apply(lambda d: feature(d), axis=1).tolist()\n",
    "y_test = (test_data['verified_purchase'] == 'Y').astype(int).tolist()\n",
    "\n",
    "# 預測\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 計算準確率\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy of the Logistic Regression Model:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "question from example file: Finding the Balanced Error Rate\n",
    "1. Compute __True__ and __False Positives__\n",
    "2. Compute __True__ and __False Negatives__\n",
    "3. Compute __Balanced Error Rate__ based on your above defined variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives (TP): 155202\n",
      "False Positives (FP): 23345\n",
      "True Negatives (TN): 1162\n",
      "False Negatives (FN): 1092\n",
      "Balanced Error Rate (BER): 0.4798\n"
     ]
    }
   ],
   "source": [
    "# 計算混淆矩陣\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "\n",
    "# True Positives, False Positives, True Negatives, False Negatives\n",
    "print(f\"True Positives (TP): {tp}\")\n",
    "print(f\"False Positives (FP): {fp}\")\n",
    "print(f\"True Negatives (TN): {tn}\")\n",
    "print(f\"False Negatives (FN): {fn}\")\n",
    "\n",
    "# 計算 Balanced Error Rate (BER)\n",
    "false_positive_rate = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "false_negative_rate = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "balanced_error_rate = (false_positive_rate + false_negative_rate) / 2\n",
    "print(f\"Balanced Error Rate (BER): {balanced_error_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Regression\n",
    "\n",
    "In this section you will start by working though two examples of altering features to further differentiate. Then you will work through how to evaluate a Regularaized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "# 使用普通文件讀取方式\n",
    "path = \"amazon_reviews_us_Musical_Instruments_v1_00.tsv\"\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    raise FileNotFoundError(f\"The file at {path} was not found.\")\n",
    "\n",
    "# 打開文件並讀取內容\n",
    "with open(path, 'r', encoding=\"utf8\") as f:\n",
    "    header = f.readline().strip().split('\\t')  # 讀取表頭\n",
    "    reg_dataset = []\n",
    "    for line in f:\n",
    "        fields = line.strip().split('\\t')\n",
    "        d = dict(zip(header, fields))\n",
    "        d['star_rating'] = int(d['star_rating'])  # 確保評分為整數\n",
    "        reg_dataset.append(d)\n",
    "\n",
    "#print(\"Data loaded successfully. Sample:\", reg_dataset[:2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 1: Unique Words in a Sample Set\n",
    "\n",
    "We are going to work with a new dataset here, as such we are going to take a smaller portion of the set and call it a Sample Set. This is because stemming on the normal training set will take a very long time. (Feel free to change sampleSet -> reg_dataset if you would like to see the difference for yourself)\n",
    "\n",
    "1. Count the number of unique words found within the 'review body' portion of the sample set defined below, making sure to __Ignore Punctuation and Capitalization__.\n",
    "2. Count the number of unique words found within the 'review body' portion of the sample set defined below, this time with use of __Stemming,__ __Ignoring Puctuation,__ ___and___ __Capitalization__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GIVEN for 1.\n",
    "wordCount = defaultdict(int)\n",
    "punctuation = set(string.punctuation)\n",
    "\n",
    "#GIVEN for 2.\n",
    "wordCountStem = defaultdict(int)\n",
    "stemmer = PorterStemmer() #use stemmer.stem(stuff)\n",
    "\n",
    "#SampleSet and y vector given\n",
    "sampleSet = reg_dataset[:2*len(reg_dataset)//10]\n",
    "y_reg = [d['star_rating'] for d in sampleSet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words (no stemming): 101381\n",
      "Number of unique words (with stemming): 83875\n"
     ]
    }
   ],
   "source": [
    "#YOUR CODE HERE\n",
    "# TODO 1: Unique Words in a Sample Set\n",
    "from collections import defaultdict\n",
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Count unique words (no stemming)\n",
    "wordCount = defaultdict(int)\n",
    "punctuation = set(string.punctuation)\n",
    "for datum in sampleSet:\n",
    "    text = datum['review_body']\n",
    "    text = ''.join([c for c in text if c not in punctuation]).lower()\n",
    "    for word in text.split():\n",
    "        wordCount[word] += 1\n",
    "unique_words_count = len(wordCount)\n",
    "print(\"Number of unique words (no stemming):\", unique_words_count)\n",
    "\n",
    "# Count unique stemmed words\n",
    "wordCountStem = defaultdict(int)\n",
    "stemmer = PorterStemmer()\n",
    "for datum in sampleSet:\n",
    "    text = datum['review_body']\n",
    "    text = ''.join([c for c in text if c not in punctuation]).lower()\n",
    "    for word in text.split():\n",
    "        stemmed_word = stemmer.stem(word)\n",
    "        wordCountStem[stemmed_word] += 1\n",
    "unique_words_count_stemmed = len(wordCountStem)\n",
    "print(\"Number of unique words (with stemming):\", unique_words_count_stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 2: Evaluating Classifiers\n",
    "\n",
    "1. Given the feature function and your counts vector, __Define__ your X_reg vector. (This being the X vector, simply labeled for the Regression model)\n",
    "2. __Fit__ your model using a __Ridge Model__ with (alpha = 1.0, fit_intercept = True).\n",
    "3. Using your model, __Make your Predictions__.\n",
    "4. Find the __MSE__ between your predictions and your y_reg vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GIVEN FUNCTIONS\n",
    "def feature_reg(datum):\n",
    "    feat = [0]*len(words)\n",
    "    r = ''.join([c for c in datum['review_body'].lower() if not c in punctuation])\n",
    "    for w in r.split():\n",
    "        if w in wordSet:\n",
    "            feat[wordId[w]] += 1\n",
    "    return feat\n",
    "\n",
    "def MSE(predictions, labels):\n",
    "    differences = [(x-y)**2 for x,y in zip(predictions,labels)]\n",
    "    return sum(differences) / len(differences)\n",
    "\n",
    "#GIVEN COUNTS AND SETS\n",
    "counts = [(wordCount[w], w) for w in wordCount]\n",
    "counts.sort()\n",
    "counts.reverse()\n",
    "\n",
    "#Note: increasing the size of the dictionary may require a lot of memory\n",
    "words = [x[1] for x in counts[:100]]\n",
    "\n",
    "wordId = dict(zip(words, range(len(words))))\n",
    "wordSet = set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 1.2041184392177153\n"
     ]
    }
   ],
   "source": [
    "#YOUR CODE HERE\n",
    "# TODO 2: Evaluating Classifiers\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Define feature vector\n",
    "X_reg = [feature_reg(d) for d in sampleSet]\n",
    "\n",
    "# Fit Ridge regression model\n",
    "model = Ridge(alpha=1.0, fit_intercept=True)\n",
    "model.fit(X_reg, y_reg)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_reg)\n",
    "\n",
    "# Compute MSE\n",
    "mse = MSE(predictions, y_reg)\n",
    "print(\"Mean Squared Error (MSE):\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Recommendation Systems\n",
    "\n",
    "For your final task, you will use your knowledge of simple latent factor-based recommender systems to make predictions. Then evaluating the performance of your predictions.\n",
    "\n",
    "### Starting up\n",
    "\n",
    "The next cell contains some starter code that you will need for your tasks in this section.\n",
    "Notice you are back to using the __trainingSet__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 1: Calculate the ratingMean\n",
    "\n",
    "1. Find the __average rating__ of your training set.\n",
    "2. Calculate a __baseline MSE value__ from the actual ratings to the average ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average rating (ratingMean): 4.25\n",
      "Baseline MSE: 1.4769\n"
     ]
    }
   ],
   "source": [
    "#YOUR CODE HERE\n",
    "# 計算平均評分\n",
    "ratingMean = train_data['star_rating'].mean()\n",
    "print(f\"Average rating (ratingMean): {ratingMean:.2f}\")\n",
    "\n",
    "# 計算基線 MSE\n",
    "def MSE(predictions, labels):\n",
    "    differences = [(x - y) ** 2 for x, y in zip(predictions, labels)]\n",
    "    return sum(differences) / len(differences)\n",
    "\n",
    "labels = train_data['star_rating'].tolist()\n",
    "baseline_predictions = [ratingMean] * len(labels)\n",
    "baseline_mse = MSE(baseline_predictions, labels)\n",
    "print(f\"Baseline MSE: {baseline_mse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are defining the functions you will need to optimize your MSE value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GIVEN\n",
    "alpha = ratingMean\n",
    "\n",
    "def prediction(user, item):\n",
    "    return alpha + userBiases[user] + itemBiases[item]\n",
    "\n",
    "def unpack(theta):\n",
    "    global alpha\n",
    "    global userBiases\n",
    "    global itemBiases\n",
    "    alpha = theta[0]\n",
    "    userBiases = dict(zip(users, theta[1:nUsers+1]))\n",
    "    itemBiases = dict(zip(items, theta[1+nUsers:]))\n",
    "    \n",
    "def cost(theta, labels, lamb):\n",
    "    unpack(theta)\n",
    "    predictions = [prediction(d['customer_id'], d['product_id']) for d in trainingSet]\n",
    "    cost = MSE(predictions, labels)\n",
    "    print(\"MSE = \" + str(cost))\n",
    "    for u in userBiases:\n",
    "        cost += lamb*userBiases[u]**2\n",
    "    for i in itemBiases:\n",
    "        cost += lamb*itemBiases[i]**2\n",
    "    return cost\n",
    "\n",
    "def derivative(theta, labels, lamb):\n",
    "    unpack(theta)\n",
    "    N = len(trainingSet)\n",
    "    dalpha = 0\n",
    "    dUserBiases = defaultdict(float)\n",
    "    dItemBiases = defaultdict(float)\n",
    "    for d in trainingSet:\n",
    "        u,i = d['customer_id'], d['product_id']\n",
    "        pred = prediction(u, i)\n",
    "        diff = pred - d['star_rating']\n",
    "        dalpha += 2/N*diff\n",
    "        dUserBiases[u] += 2/N*diff\n",
    "        dItemBiases[i] += 2/N*diff\n",
    "    for u in userBiases:\n",
    "        dUserBiases[u] += 2*lamb*userBiases[u]\n",
    "    for i in itemBiases:\n",
    "        dItemBiases[i] += 2*lamb*itemBiases[i]\n",
    "    dtheta = [dalpha] + [dUserBiases[u] for u in users] + [dItemBiases[i] for i in items]\n",
    "    return numpy.array(dtheta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 2: Optimize\n",
    "\n",
    "1. __Optimize__ your MSE using the scipy.optimize.fmin_1_bfgs_b(\"arguments\") functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 1.476904941435598\n",
      "MSE = 1.4658348300504576\n",
      "MSE = 1.476190268173242\n",
      "MSE = 1.4761932521290224\n",
      "MSE = 1.476190086408308\n",
      "MSE = 1.4761900730469077\n",
      "MSE = 1.4761899647366807\n",
      "Optimized MSE: 1.4765\n"
     ]
    }
   ],
   "source": [
    "#YOUR CODE HERE\n",
    "users = train_data['customer_id'].unique()\n",
    "items = train_data['product_id'].unique()\n",
    "nUsers = len(users)\n",
    "nItems = len(items)\n",
    "\n",
    "# 初始化偏差\n",
    "userBiases = {u: 0 for u in users}\n",
    "itemBiases = {i: 0 for i in items}\n",
    "theta = [ratingMean] + [0] * (nUsers + nItems)\n",
    "\n",
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "from collections import defaultdict\n",
    "\n",
    "# 訓練集格式轉換\n",
    "trainingSet = train_data.to_dict(orient='records')\n",
    "\n",
    "# 設定正則化參數\n",
    "lamb = 0.1\n",
    "\n",
    "# 優化\n",
    "optimized_theta, final_cost, info = fmin_l_bfgs_b(\n",
    "    func=cost, \n",
    "    x0=theta, \n",
    "    fprime=derivative, \n",
    "    args=(labels, lamb), \n",
    "    maxiter=100\n",
    ")\n",
    "\n",
    "print(f\"Optimized MSE: {final_cost:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finished!\n",
    "\n",
    "Congratulations! You are now ready to submit your work. Once you have submitted make sure to get started on your peer reviews!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
